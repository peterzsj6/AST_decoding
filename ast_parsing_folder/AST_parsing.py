import csv
from statistics import mean
import os
try:
    import tree_sitter_python
except Exception:
    tree_sitter_python = None
try:
    import tree_sitter_java
except Exception:
    tree_sitter_java = None
try:
    import tree_sitter_javascript
except Exception:
    tree_sitter_javascript = None
try:
    import tree_sitter_php
except Exception:
    tree_sitter_php = None
try:
    import tree_sitter_rust
except Exception:
    tree_sitter_rust = None
try:
    import tree_sitter_cpp
except Exception:
    tree_sitter_cpp = None

from tree_sitter import Language, Parser

# Path to your built shared library
LIB_PATH = "/data/home/zhangsj/lora_code/ast_parsing_folder/my-languages.so"

# Only register languages that successfully imported
LANGUAGES = {}
if tree_sitter_python is not None:
    LANGUAGES["python"] = tree_sitter_python
if tree_sitter_java is not None:
    LANGUAGES["java"] = tree_sitter_java
if tree_sitter_javascript is not None:
    LANGUAGES["javascript"] = tree_sitter_javascript
if tree_sitter_php is not None:
    LANGUAGES["php"] = tree_sitter_php
if tree_sitter_rust is not None:
    LANGUAGES["rust"] = tree_sitter_rust
if tree_sitter_cpp is not None:
    LANGUAGES["cpp"] = tree_sitter_cpp

def preprocess_code_for_parsing(code, ps_language="python"):
    """
    Preprocess code to handle special tokens and improve parsing success.
    
    Args:
        code: Original code string
        ps_language: Programming language
    
    Returns:
        Preprocessed code string
    """
    import re
    
    if not code or not isinstance(code, str):
        return code
    
    # Remove or replace common problematic tokens
    problematic_patterns = [
        # Instruction-following tokens
        (r'<\|im_start\|>', ''),
        (r'<\|im_end\|>', ''),
        (r'<\|assistant\|>', ''),
        (r'<\|user\|>', ''),
        (r'<\|system\|>', ''),
        (r'<\|end\|>', ''),
        # Other common problematic patterns
        (r'<\|.*?\|>', ''),  # Any remaining <|...|> patterns
        # Remove error messages that got mixed into code
        (r'Error at \w+:', ''),
        (r'Error at ERROR:', ''),
    ]
    
    preprocessed_code = code
    for pattern, replacement in problematic_patterns:
        preprocessed_code = re.sub(pattern, replacement, preprocessed_code)
    
    # Language-specific preprocessing
    if ps_language == "python":
        # Handle common Python-specific issues
        # Remove lines that contain error messages or problematic tokens
        lines = preprocessed_code.split('\n')
        cleaned_lines = []
        for line in lines:
            stripped_line = line.strip()
            # Skip problematic lines
            if (stripped_line and 
                not stripped_line.startswith('Error at') and
                not stripped_line.startswith('<|') and 
                not stripped_line.endswith('|>') and
                not stripped_line in ['<|', '|>', '<|>', 'im_end', 'im_start'] and
                not any(token in stripped_line for token in ['<|', '|>', 'Error at'])):
                cleaned_lines.append(line)
        preprocessed_code = '\n'.join(cleaned_lines)
        
        # Clean up extra whitespace
        preprocessed_code = re.sub(r'\n\s*\n\s*\n', '\n\n', preprocessed_code)  # Remove excessive blank lines
        preprocessed_code = preprocessed_code.strip()
    
    return preprocessed_code

def validate_code_syntax(code, ps_language="python"):
    """
    Basic syntax validation for code before AST parsing.
    
    Args:
        code: Code string to validate
        ps_language: Programming language
    
    Returns:
        (is_valid, issues) tuple
    """
    issues = []
    
    if not code or not isinstance(code, str):
        return False, ["Invalid or empty code"]
    
    if ps_language == "python":
        # Basic Python syntax checks
        lines = code.split('\n')
        
        # Check for basic structure
        has_function_def = any('def ' in line for line in lines)
        has_class_def = any('class ' in line for line in lines)
        has_meaningful_content = any(line.strip() and not line.strip().startswith('#') for line in lines)
        
        if not has_meaningful_content:
            issues.append("No meaningful code content found")
        
        # Check for unmatched parentheses/brackets
        open_chars = {'(': ')', '[': ']', '{': '}'}
        stack = []
        for char in code:
            if char in open_chars:
                stack.append(open_chars[char])
            elif char in open_chars.values():
                if not stack or stack.pop() != char:
                    issues.append("Unmatched brackets/parentheses")
                    break
        
        if stack:
            issues.append("Unclosed brackets/parentheses")
        
        # Check for basic Python syntax requirements
        try:
            compile(code, '<string>', 'exec')
        except SyntaxError as e:
            issues.append(f"Python syntax error: {e}")
        except Exception:
            # Other compilation errors are OK for AST parsing
            pass
    
    return len(issues) == 0, issues

def parse_to_ast_with_fallback(code, ps_language="python", verbose_errors=False):
    """
    Parse code to AST with multiple fallback strategies for maximum success rate.
    
    Args:
        code: Code string to parse
        ps_language: Programming language
        verbose_errors: Whether to print detailed error information
    
    Returns:
        (ast_root, strategy_used) tuple
    """
    strategies = [
        ("preprocessed", lambda: parse_to_ast(code, ps_language, verbose_errors=False, preprocess=True, validate_syntax=True)),
        ("preprocessed_no_validation", lambda: parse_to_ast(code, ps_language, verbose_errors=False, preprocess=True, validate_syntax=False)),
        ("raw", lambda: parse_to_ast(code, ps_language, verbose_errors=False, preprocess=False, validate_syntax=False)),
        ("aggressive_clean", lambda: parse_to_ast(
            preprocess_code_for_parsing(code, ps_language), 
            ps_language, 
            verbose_errors=False, 
            preprocess=False, 
            validate_syntax=False
        ))
    ]
    
    for strategy_name, strategy_func in strategies:
        try:
            ast_root = strategy_func()
            if ast_root is not None:
                if verbose_errors:
                    print(f"Successfully parsed using strategy: {strategy_name}")
                return ast_root, strategy_name
        except Exception as e:
            if verbose_errors:
                print(f"Strategy '{strategy_name}' failed: {e}")
            continue
    
    if verbose_errors:
        print(f"All parsing strategies failed for {ps_language}")
    
    return None, "failed"

def parse_to_ast(code, ps_language="python", verbose_errors=False, preprocess=True, validate_syntax=True):
    """
    Parse code to AST for the specified language with comprehensive error handling.
    
    Args:
        code: Code string to parse
        ps_language: Programming language ("python", "java", etc.)
        verbose_errors: Whether to print detailed error information
        preprocess: Whether to preprocess code to handle special tokens
        validate_syntax: Whether to validate syntax before parsing
    
    Returns:
        AST root node or None if parsing fails
    """
    import re
    
    if not code or not isinstance(code, str):
        if verbose_errors:
            print(f"Error: Invalid code input for {ps_language}")
        return None
    
    original_code = code
    
    # Preprocess code if requested
    if preprocess:
        code = preprocess_code_for_parsing(code, ps_language)
        
        # Check if preprocessing removed too much
        if len(code.strip()) < len(original_code.strip()) * 0.3:  # If more than 70% was removed
            if verbose_errors:
                print(f"Warning: Preprocessing removed significant content for {ps_language}")
                print(f"Original length: {len(original_code)}, Preprocessed length: {len(code)}")
    
    # Validate syntax if requested
    if validate_syntax:
        is_valid, issues = validate_code_syntax(code, ps_language)
        if not is_valid and verbose_errors:
            print(f"Warning: Code validation issues for {ps_language}: {issues}")
    
    try:
        # Validate language support
        if ps_language not in LANGUAGES:
            if verbose_errors:
                print(f"Error: Unsupported language '{ps_language}'. Supported: {list(LANGUAGES.keys())}")
            return None
        
        # Get language parser
        if ps_language == "php":
            p_language = Language(LANGUAGES[ps_language].language_php())
        else:
            p_language = Language(LANGUAGES[ps_language].language())
        
        parser = Parser(p_language)
        tree = parser.parse(bytes(code, "utf8"))
        
        # Check for parsing errors with improved filtering
        if tree.root_node.has_error:
            error_nodes = []
            
            def collect_errors(node):
                if node.has_error:
                    node_text = node.text.decode('utf-8', errors='ignore')
                    # Filter out known problematic patterns
                    problematic_patterns = ['<|', '|>', 'im_start', 'im_end', 'assistant', 'Error at']
                    if not any(pattern in node_text for pattern in problematic_patterns):
                        error_nodes.append((node.type, node_text[:100]))
                for child in node.children:
                    collect_errors(child)
            
            collect_errors(tree.root_node)
            
            if verbose_errors:
                if error_nodes:
                    print(f"Warning: {len(error_nodes)} parsing errors found for {ps_language}:")
                    for error_type, error_text in error_nodes[:5]:  # Show first 5 errors
                        print(f"  - {error_type}: {error_text}...")
                    if len(error_nodes) > 5:
                        print(f"  ... and {len(error_nodes) - 5} more errors")
                else:
                    print(f"Info: Parsing errors were filtered out (likely special tokens)")
            elif error_nodes:
                print(f"Warning: {len(error_nodes)} parsing errors found for {ps_language} (use verbose_errors=True for details)")
        
        return tree.root_node
        
    except Exception as e:
        if verbose_errors:
            print(f"Error parsing {ps_language} code: {e}")
            import traceback
            traceback.print_exc()
        else:
            print(f"Error parsing {ps_language} code: {str(e)[:100]}...")
        return None

def print_ast_structure(node, depth=0):
    """Print the full AST structure for debugging"""
    indent = "  " * depth
    node_text = node.text.decode('utf-8').replace('\n', '\\n')
    print(f"{indent}{node.type}: '{node_text}'")
    for child in node.children:
        print_ast_structure(child, depth + 1)

def is_processable_node(node):
    """Check if the node should be processed (including comments and string literals)"""
    return True  # Process all nodes

def analyze_ast_extraction_quality(nodes):
    """
    Analyze the quality of AST extraction results
    
    Args:
        nodes: List of AST nodes from get_ast_leaf_nodes
    
    Returns:
        Dictionary with analysis metrics
    """
    if not nodes:
        return {"error": "No nodes found"}
    
    # Count node types
    type_counts = {}
    text_lengths = []
    depths = []
    
    for node in nodes:
        node_type = node['type']
        type_counts[node_type] = type_counts.get(node_type, 0) + 1
        
        text_lengths.append(len(node['text']))
        depths.append(node['depth'])
    
    # Calculate metrics
    total_nodes = len(nodes)
    unique_types = len(type_counts)
    avg_text_length = sum(text_lengths) / len(text_lengths) if text_lengths else 0
    max_depth = max(depths) if depths else 0
    avg_depth = sum(depths) / len(depths) if depths else 0
    
    # Check for important node types
    important_types = {'identifier', 'keyword', 'operator', 'comment', 'string'}
    covered_important_types = important_types.intersection(set(type_counts.keys()))
    
    return {
        "total_nodes": total_nodes,
        "unique_node_types": unique_types,
        "node_type_distribution": type_counts,
        "average_text_length": avg_text_length,
        "max_depth": max_depth,
        "average_depth": avg_depth,
        "important_types_covered": len(covered_important_types),
        "important_types_missing": len(important_types - covered_important_types),
        "coverage_score": len(covered_important_types) / len(important_types)
    }

def get_ast_leaf_nodes_simple(node, nodes=None):
    """Backward compatibility function that returns the old format [type, text]"""
    detailed_nodes = get_ast_leaf_nodes(node, nodes, include_positions=False)
    return [[node['type'], node['text']] for node in detailed_nodes]

def get_span_token_positions_simple(code_text, ast_spans, tokenizer_path="/data/home/zhangsj/qwen_coder_1.5b"):
    """
    Get token positions for each AST span in simple list format.
    
    Args:
        code_text: Original code text
        ast_spans: List of AST leaf nodes from get_ast_leaf_nodes_for_spans
        tokenizer_path: Path to the tokenizer
    
    Returns:
        List of lists, where each sublist contains token positions for a span
        Format: [[token_pos1, token_pos2, ...], [token_pos3, token_pos4, ...], ...]
    """
    # If the spans already have token_indices (from new method), use them directly
    if ast_spans and 'token_indices' in ast_spans[0]:
        return [span['token_indices'] for span in ast_spans]
    
    # Otherwise fall back to the old method
    detailed_spans = get_span_token_positions(code_text, ast_spans, tokenizer_path)
    
    # Extract just the token positions as list of lists
    span_token_lists = []
    for span in detailed_spans:
        span_token_lists.append(span.get('token_positions', []))
    
    return span_token_lists

def get_span_token_positions(code_text, ast_spans, tokenizer_path="/data/home/zhangsj/qwen_coder_1.5b"):
    """
    Get token positions for each AST span using robust character-to-token mapping.
    
    Args:
        code_text: Original code text
        ast_spans: List of AST leaf nodes from get_ast_leaf_nodes_for_spans
        tokenizer_path: Path to the tokenizer
    
    Returns:
        List of span dictionaries with token positions and metadata
    """
    # If spans already have token_indices (from new token-aligned method), use them
    if ast_spans and 'token_indices' in ast_spans[0]:
        print(f"Using pre-computed token indices from {len(ast_spans)} spans")
        span_token_positions = []
        
        for span_idx, span in enumerate(ast_spans):
            token_positions = span['token_indices']
            print(f"Span {span_idx}: '{span['text']}' -> tokens {token_positions}")
            
            span_token_positions.append({
                'span_type': span['type'],
                'span_text': span['text'],
                'token_positions': token_positions,
                'start_line': span.get('start_line', 0),
                'start_column': span.get('start_column', 0),
                'end_line': span.get('end_line', 0),
                'end_column': span.get('end_column', 0)
            })
        
        return span_token_positions
    
    # Legacy method for spans without pre-computed token indices
    print("Falling back to legacy token position calculation...")
    
    # Load tokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        print(f"Tokenizer loaded from {tokenizer_path}")
    except Exception as e:
        print(f"Error loading tokenizer: {e}")
        return []
    
    # Tokenize the entire code with return_offsets_mapping for accurate alignment
    if hasattr(tokenizer, 'encode_plus'):
        encoding = tokenizer.encode_plus(
            code_text,
            add_special_tokens=False,
            return_offsets_mapping=True,
            return_tensors=None
        )
        tokens = encoding['input_ids']
        offset_mapping = encoding['offset_mapping']
    else:
        # Fallback for tokenizers without offset mapping
        tokens = tokenizer.encode(code_text, add_special_tokens=False)
        offset_mapping = None
    
    token_texts = tokenizer.convert_ids_to_tokens(tokens)
    
    print(f"Total tokens: {len(tokens)}")
    print(f"Code length: {len(code_text)} characters")
    
    span_token_positions = []
    
    for span_idx, span in enumerate(ast_spans):
        span_text = span['text']
        start_line = span.get('start_line', 0)
        start_column = span.get('start_column', 0)
        end_line = span.get('end_line', 0)
        end_column = span.get('end_column', 0)
        
        # Convert line/column positions to character positions
        lines = code_text.split('\n')
        
        # Calculate character positions more accurately
        start_char = 0
        for i in range(start_line):
            start_char += len(lines[i]) + 1  # +1 for newline
        start_char += start_column
        
        end_char = 0
        for i in range(end_line):
            end_char += len(lines[i]) + 1  # +1 for newline
        end_char += end_column
        
        print(f"Span {span_idx}: '{span_text}' at lines {start_line}:{start_column}-{end_line}:{end_column} (chars {start_char}-{end_char})")
        print(f"  Text at this position: '{code_text[start_char:end_char]}'")
        
        # Find tokens using offset mapping if available
        token_positions = []
        
        if offset_mapping is not None:
            # Use offset mapping for precise alignment
            for i, (token_start, token_end) in enumerate(offset_mapping):
                # Check if token overlaps with span
                if (token_start < end_char and token_end > start_char):
                    token_positions.append(i)
                    print(f"  Found token {i}: '{token_texts[i]}' at chars {token_start}-{token_end}")
        else:
            # Fallback: use approximate matching
            token_positions = find_tokens_by_text_matching(
                code_text, tokens, token_texts, tokenizer, start_char, end_char, span_text
            )
        
        if token_positions:
            print(f"  -> Token positions: {token_positions}")
            span_token_positions.append({
                'span_type': span['type'],
                'span_text': span_text,
                'token_positions': token_positions,
                'start_line': start_line,
                'start_column': start_column,
                'end_line': end_line,
                'end_column': end_column,
                'start_char': start_char,
                'end_char': end_char
            })
        else:
            print(f"  -> FAILED to find token positions!")
    
    return span_token_positions

def find_tokens_by_text_matching(code_text, tokens, token_texts, tokenizer, start_char, end_char, span_text):
    """
    Fallback method to find token positions when offset mapping is not available.
    Uses approximate text matching based on token reconstruction.
    """
    token_positions = []
    
    # Try to find the span text in the reconstructed token sequence
    reconstructed_text = ""
    char_to_token_map = {}
    
    for i, token_id in enumerate(tokens):
        token_str = tokenizer.decode([token_id])
        token_start = len(reconstructed_text)
        reconstructed_text += token_str
        token_end = len(reconstructed_text)
        
        # Map character positions to token indices
        for char_pos in range(token_start, token_end):
            if char_pos < len(char_to_token_map) + token_start:
                char_to_token_map[char_pos] = i
    
    # Find tokens that overlap with the span
    for char_pos in range(start_char, end_char):
        if char_pos in char_to_token_map:
            token_idx = char_to_token_map[char_pos]
            if token_idx not in token_positions:
                token_positions.append(token_idx)
    
    return sorted(token_positions)

def convert_spans_to_token_positions(tokenizer, code_text, ast_spans):
    """
    Convert AST spans to token positions for SpanBERT-style training.
    
    Args:
        tokenizer: The tokenizer to use for tokenization
        code_text: Original code text
        ast_spans: List of AST leaf nodes from get_ast_leaf_nodes_for_spans
    
    Returns:
        List of span dictionaries with token positions and metadata
    """
    # Tokenize the entire code
    tokens = tokenizer.encode(code_text, add_special_tokens=False)
    token_texts = tokenizer.convert_ids_to_tokens(tokens)
    
    span_token_positions = []
    
    for span in ast_spans:
        span_text = span['text']
        
        # Find all occurrences of this span in the tokenized text
        span_occurrences = []
        
        # Simple approach: find span text in original text and map to tokens
        start_char = 0
        while True:
            pos = code_text.find(span_text, start_char)
            if pos == -1:
                break
            
            # Find which tokens this span corresponds to
            span_start_token = None
            span_end_token = None
            
            # Approximate token mapping (this is a simplified approach)
            char_count = 0
            for i, token in enumerate(tokens):
                token_text = tokenizer.decode([token]).strip()
                if char_count <= pos < char_count + len(token_text):
                    if span_start_token is None:
                        span_start_token = i
                if char_count <= pos + len(span_text) <= char_count + len(token_text):
                    span_end_token = i + 1
                    break
                char_count += len(token_text)
            
            if span_start_token is not None and span_end_token is not None:
                span_occurrences.append({
                    'start_token': span_start_token,
                    'end_token': span_end_token,
                    'token_count': span_end_token - span_start_token
                })
            
            start_char = pos + 1
        
        # Add span with token positions
        if span_occurrences:
            for occurrence in span_occurrences:
                span_token_positions.append({
                    'span_type': span['type'],
                    'span_text': span_text,
                    'start_token': occurrence['start_token'],
                    'end_token': occurrence['end_token'],
                    'token_count': occurrence['token_count'],
                    'start_line': span.get('start_line', 0),
                    'start_column': span.get('start_column', 0),
                    'end_line': span.get('end_line', 0),
                    'end_column': span.get('end_column', 0)
                })
    
    return span_token_positions

def validate_comprehensive_coverage(nodes, code_text, tokenizer_path="/data/home/zhangsj/qwen_coder_1.5b"):
    """
    Comprehensive validation of AST span coverage against tokenizer output.
    
    Args:
        nodes: List of AST nodes from get_ast_leaf_nodes_for_spans
        code_text: Original code text
        tokenizer_path: Path to the tokenizer
    
    Returns:
        Dictionary with detailed validation results
    """
    if not nodes:
        return {"error": "No nodes found"}
    
    # Load tokenizer for token-level validation
    try:
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    except Exception as e:
        print(f"Warning: Could not load tokenizer for validation: {e}")
        tokenizer = None
    
    # Check if we have token-aligned spans (new method)
    has_token_indices = nodes and 'token_indices' in nodes[0]
    
    if has_token_indices:
        # Use precise token-level validation for new method
        return validate_token_aligned_coverage(nodes, code_text, tokenizer)
    else:
        # Fall back to character-level validation for legacy method
        return validate_character_level_coverage(nodes, code_text, tokenizer)

def validate_token_aligned_coverage(nodes, code_text, tokenizer):
    """Validate coverage for token-aligned spans."""
    if not tokenizer:
        return {"error": "Tokenizer required for token-aligned validation"}
    
    # Get total tokens
    tokens = tokenizer.encode(code_text, add_special_tokens=False)
    total_tokens = len(tokens)
    
    # Check which tokens are covered
    covered_tokens = set()
    overlapping_tokens = set()
    
    for span in nodes:
        token_indices = span.get('token_indices', [])
        for token_idx in token_indices:
            if token_idx in covered_tokens:
                overlapping_tokens.add(token_idx)
            covered_tokens.add(token_idx)
    
    # Calculate coverage metrics
    coverage_percentage = (len(covered_tokens) / total_tokens) * 100 if total_tokens > 0 else 0
    uncovered_tokens = set(range(total_tokens)) - covered_tokens
    
    # Analyze span quality
    span_lengths = [len(span['text']) for span in nodes]
    node_types = [span['type'] for span in nodes]
    type_distribution = {}
    for node_type in node_types:
        type_distribution[node_type] = type_distribution.get(node_type, 0) + 1
    
    # Analyze uncovered tokens
    uncovered_analysis = {"message": "Perfect token coverage"} if not uncovered_tokens else {
        "uncovered_token_count": len(uncovered_tokens),
        "uncovered_token_sample": list(uncovered_tokens)[:10]
    }
    
    return {
        "total_spans": len(nodes),
        "total_tokens": total_tokens,
        "covered_tokens": len(covered_tokens),
        "uncovered_tokens": len(uncovered_tokens),
        "overlapping_tokens": len(overlapping_tokens),
        "token_coverage_percentage": coverage_percentage,
        "gap_analysis": uncovered_analysis,
        "token_analysis": {
            "total_tokens": total_tokens,
            "covered_tokens": len(covered_tokens),
            "token_coverage_percentage": coverage_percentage,
            "has_offset_mapping": True
        },
        "span_statistics": {
            "average_length": sum(span_lengths) / len(span_lengths) if span_lengths else 0,
            "min_length": min(span_lengths) if span_lengths else 0,
            "max_length": max(span_lengths) if span_lengths else 0,
            "type_distribution": type_distribution
        },
        "is_complete_coverage": len(uncovered_tokens) == 0,
        "quality_score": calculate_token_coverage_quality_score(coverage_percentage, type_distribution, len(overlapping_tokens))
    }

def validate_character_level_coverage(nodes, code_text, tokenizer):
    """Legacy character-level validation for old spans."""
    # Sort nodes by position
    sorted_nodes = sorted(nodes, key=lambda x: (x.get('start_line', 0), x.get('start_column', 0)))
    
    # Character-level coverage analysis
    total_chars = len(code_text)
    covered_chars = set()
    char_to_spans = {}  # Map character positions to spans
    
    lines = code_text.split('\n')
    
    # Map each span to character positions
    for span_idx, node in enumerate(sorted_nodes):
        start_line = node.get('start_line', 0)
        start_col = node.get('start_column', 0)
        end_line = node.get('end_line', 0)
        end_col = node.get('end_column', 0)
        
        # Calculate character positions
        start_char = sum(len(lines[i]) + 1 for i in range(start_line)) + start_col
        end_char = sum(len(lines[i]) + 1 for i in range(end_line)) + end_col
        
        # Adjust for the first line (no leading newline)
        if start_line > 0:
            start_char -= 1
        if end_line > 0:
            end_char -= 1
        
        # Mark covered characters and detect overlaps
        for char_pos in range(start_char, min(end_char, total_chars)):
            char_to_spans[char_pos] = span_idx
            covered_chars.add(char_pos)
    
    # Find uncovered characters
    uncovered_chars = []
    for i in range(total_chars):
        if i not in covered_chars:
            uncovered_chars.append(i)
    
    # Analyze uncovered characters
    gap_analysis = analyze_uncovered_chars(code_text, uncovered_chars)
    
    # Token-level analysis if tokenizer is available
    token_analysis = {}
    if tokenizer:
        token_analysis = analyze_token_coverage(code_text, sorted_nodes, tokenizer)
    
    # Coverage statistics
    coverage_percentage = (len(covered_chars) / total_chars) * 100 if total_chars > 0 else 0
    
    # Span quality metrics
    span_lengths = [len(node['text']) for node in sorted_nodes]
    node_types = [node['type'] for node in sorted_nodes]
    type_distribution = {}
    for node_type in node_types:
        type_distribution[node_type] = type_distribution.get(node_type, 0) + 1
    
    return {
        "total_spans": len(sorted_nodes),
        "total_characters": total_chars,
        "covered_characters": len(covered_chars),
        "uncovered_characters": len(uncovered_chars),
        "coverage_percentage": coverage_percentage,
        "gap_analysis": gap_analysis,
        "token_analysis": token_analysis,
        "span_statistics": {
            "average_length": sum(span_lengths) / len(span_lengths) if span_lengths else 0,
            "min_length": min(span_lengths) if span_lengths else 0,
            "max_length": max(span_lengths) if span_lengths else 0,
            "type_distribution": type_distribution
        },
        "is_complete_coverage": len(uncovered_chars) == 0,
        "quality_score": calculate_coverage_quality_score(coverage_percentage, gap_analysis, type_distribution)
    }

def calculate_token_coverage_quality_score(coverage_percentage, type_distribution, overlap_count):
    """Calculate quality score for token-aligned coverage."""
    base_score = coverage_percentage
    
    # Bonus for good type diversity
    important_types = {'keyword', 'identifier', 'operator', 'string', 'number', 'comment'}
    covered_important = set(type_distribution.keys()).intersection(important_types)
    type_coverage_bonus = (len(covered_important) / len(important_types)) * 10
    
    # Penalty for overlapping tokens
    overlap_penalty = overlap_count * 0.5
    
    quality_score = base_score + type_coverage_bonus - overlap_penalty
    return max(0, min(100, quality_score))

def analyze_uncovered_chars(code_text, uncovered_chars):
    """Analyze what types of characters are not covered by spans."""
    if not uncovered_chars:
        return {"message": "Perfect coverage - no uncovered characters"}
    
    uncovered_text = ""
    whitespace_count = 0
    newline_count = 0
    punctuation_count = 0
    other_count = 0
    
    for char_pos in uncovered_chars:
        if char_pos < len(code_text):
            char = code_text[char_pos]
            uncovered_text += char
            
            if char.isspace():
                if char == '\n':
                    newline_count += 1
                else:
                    whitespace_count += 1
            elif char in '()[]{},.;:':
                punctuation_count += 1
            else:
                other_count += 1
    
    return {
        "total_uncovered": len(uncovered_chars),
        "whitespace_count": whitespace_count,
        "newline_count": newline_count,
        "punctuation_count": punctuation_count,
        "other_count": other_count,
        "uncovered_sample": uncovered_text[:100] + "..." if len(uncovered_text) > 100 else uncovered_text
    }

def analyze_token_coverage(code_text, ast_spans, tokenizer):
    """Analyze coverage from a tokenizer perspective."""
    try:
        # Get tokenizer output with offsets if available
        if hasattr(tokenizer, 'encode_plus'):
            encoding = tokenizer.encode_plus(
                code_text,
                add_special_tokens=False,
                return_offsets_mapping=True,
                return_tensors=None
            )
            tokens = encoding['input_ids']
            offset_mapping = encoding.get('offset_mapping', None)
        else:
            tokens = tokenizer.encode(code_text, add_special_tokens=False)
            offset_mapping = None
        
        total_tokens = len(tokens)
        covered_tokens = set()
        
        if offset_mapping:
            # Use precise offset mapping
            for span in ast_spans:
                span_start = calculate_char_position(code_text, span['start_line'], span['start_column'])
                span_end = calculate_char_position(code_text, span['end_line'], span['end_column'])
                
                for token_idx, (token_start, token_end) in enumerate(offset_mapping):
                    if token_start < span_end and token_end > span_start:
                        covered_tokens.add(token_idx)
        
        token_coverage = (len(covered_tokens) / total_tokens) * 100 if total_tokens > 0 else 0
        
        return {
            "total_tokens": total_tokens,
            "covered_tokens": len(covered_tokens),
            "token_coverage_percentage": token_coverage,
            "has_offset_mapping": offset_mapping is not None
        }
        
    except Exception as e:
        return {"error": f"Token analysis failed: {e}"}

def calculate_char_position(code_text, line, column):
    """Calculate character position from line/column coordinates."""
    lines = code_text.split('\n')
    char_pos = sum(len(lines[i]) + 1 for i in range(line)) + column
    if line > 0:
        char_pos -= 1  # Adjust for no leading newline on first line
    return char_pos

def calculate_coverage_quality_score(coverage_percentage, gap_analysis, type_distribution):
    """Calculate a quality score for the coverage."""
    base_score = coverage_percentage
    
    # Penalty for missing important types
    important_types = {'keyword', 'identifier', 'operator', 'string', 'number'}
    covered_important = set(type_distribution.keys()).intersection(important_types)
    type_coverage_bonus = (len(covered_important) / len(important_types)) * 10
    
    # Penalty for too many uncovered non-whitespace characters
    if 'other_count' in gap_analysis:
        other_penalty = gap_analysis['other_count'] * 2
    else:
        other_penalty = 0
    
    quality_score = base_score + type_coverage_bonus - other_penalty
    return max(0, min(100, quality_score))

def validate_spans_for_spanbert(nodes, code_text):
    """
    Legacy validation function - now wraps the comprehensive validation.
    """
    comprehensive_result = validate_comprehensive_coverage(nodes, code_text)
    
    # Convert to legacy format for backward compatibility
    return {
        "total_spans": comprehensive_result.get("total_spans", 0),
        "unique_node_types": len(comprehensive_result.get("span_statistics", {}).get("type_distribution", {})),
        "overlaps_found": 0,  # New system handles overlaps differently
        "overlap_details": [],
        "uncovered_characters": comprehensive_result.get("uncovered_characters", 0),
        "coverage_percentage": comprehensive_result.get("coverage_percentage", 0),
        "average_span_length": comprehensive_result.get("span_statistics", {}).get("average_length", 0),
        "span_length_distribution": {
            "min": comprehensive_result.get("span_statistics", {}).get("min_length", 0),
            "max": comprehensive_result.get("span_statistics", {}).get("max_length", 0),
            "median": 0  # Would need to calculate from spans
        },
        "is_valid_for_spanbert": comprehensive_result.get("is_complete_coverage", False)
    }

def get_ast_leaf_nodes_for_spans(node, nodes=None, include_positions=True):
    """
    Get comprehensive AST nodes for complete token coverage using a token-first approach.
    
    This function ensures:
    1. Complete coverage - ALL tokens in the code are covered
    2. No overlapping spans - each token belongs to exactly one span  
    3. Token-aligned spans that work with any tokenizer
    
    Args:
        node: AST node to process
        nodes: Accumulated list of nodes
        include_positions: Whether to include position information
    
    Returns:
        List of node dictionaries with type, text, and position info
    """
    if nodes is None:
        nodes = []
    
    # Use the new token-first approach for guaranteed complete coverage
    from transformers import AutoTokenizer
    
    # Get the original code text
    code_text = node.text.decode('utf-8')
    
    # Use token-first approach for complete coverage
    return get_token_aligned_ast_spans(code_text, node, include_positions)

def get_comprehensive_ast_spans(node, nodes=None, include_positions=True, processed_ranges=None):
    """
    Extract comprehensive non-overlapping spans that cover all tokens.
    
    Strategy:
    1. Process nodes in a way that ensures complete coverage
    2. Prefer meaningful semantic units over pure leaf nodes
    3. Handle whitespace and punctuation properly
    4. Ensure no overlaps or gaps
    """
    if nodes is None:
        nodes = []
    if processed_ranges is None:
        processed_ranges = set()
    
    # Define node types that should always be treated as atomic spans
    atomic_types = {
        'string', 'integer', 'float', 'boolean', 'null', 'none',
        'identifier', 'comment', 'true', 'false'
    }
    
    # Define structural types that may need special handling
    structural_types = {
        'function_def', 'class_def', 'if_statement', 'for_statement', 
        'while_statement', 'assignment', 'call'
    }
    
    # Current node's range
    node_start = node.start_point
    node_end = node.end_point
    node_range = (node_start[0], node_start[1], node_end[0], node_end[1])
    
    # Skip if this range is already processed
    if node_range in processed_ranges:
        return nodes
    
    # If this is a leaf node or atomic type, add it as a span
    if len(node.children) == 0 or node.type in atomic_types:
        node_text = node.text.decode('utf-8').strip()
        if node_text:  # Only add non-empty nodes
            node_info = {
                'type': node.type,
                'text': node_text
            }
            
            if include_positions:
                node_info.update({
                    'start_line': node.start_point[0],
                    'start_column': node.start_point[1],
                    'end_line': node.end_point[0],
                    'end_column': node.end_point[1]
                })
            
            nodes.append(node_info)
            processed_ranges.add(node_range)
    else:
        # For non-leaf nodes, we need to carefully handle gaps between children
        child_ranges = []
        
        # Collect all child ranges
        for child in node.children:
            child_start = child.start_point
            child_end = child.end_point  
            child_ranges.append((child_start, child_end, child))
        
        # Sort children by position
        child_ranges.sort(key=lambda x: (x[0][0], x[0][1]))
        
        # Process gaps between children (these contain keywords, operators, punctuation)
        node_text = node.text.decode('utf-8')
        last_end = node.start_point
        
        for child_start, child_end, child in child_ranges:
            # Check for gap between last_end and child_start
            if (last_end[0] < child_start[0] or 
                (last_end[0] == child_start[0] and last_end[1] < child_start[1])):
                
                # Extract gap text
                gap_text = extract_text_between_points(node_text, node.start_point, last_end, child_start)
                if gap_text.strip():
                    # Split gap into meaningful tokens
                    gap_spans = tokenize_gap_text(gap_text, last_end, child_start)
                    for gap_span in gap_spans:
                        if include_positions:
                            gap_span.update({
                                'start_line': gap_span.get('start_line', last_end[0]),
                                'start_column': gap_span.get('start_column', last_end[1]),
                                'end_line': gap_span.get('end_line', child_start[0]),
                                'end_column': gap_span.get('end_column', child_start[1])
                            })
                        nodes.append(gap_span)
            
            # Recursively process child
            get_comprehensive_ast_spans(child, nodes, include_positions, processed_ranges)
            last_end = child_end
        
        # Handle gap after last child to node end
        if (last_end[0] < node.end_point[0] or
            (last_end[0] == node.end_point[0] and last_end[1] < node.end_point[1])):
            
            gap_text = extract_text_between_points(node_text, node.start_point, last_end, node.end_point)
            if gap_text.strip():
                gap_spans = tokenize_gap_text(gap_text, last_end, node.end_point)
                for gap_span in gap_spans:
                    if include_positions:
                        gap_span.update({
                            'start_line': gap_span.get('start_line', last_end[0]),
                            'start_column': gap_span.get('start_column', last_end[1]), 
                            'end_line': gap_span.get('end_line', node.end_point[0]),
                            'end_column': gap_span.get('end_column', node.end_point[1])
                        })
                    nodes.append(gap_span)
    
    return nodes

def extract_text_between_points(full_text, node_start, start_point, end_point):
    """Extract text between two points in the AST."""
    # Convert points to character positions
    lines = full_text.split('\n')
    
    # Calculate start character position
    start_char = 0
    for i in range(start_point[0] - node_start[0]):
        if i + node_start[0] < len(lines):
            start_char += len(lines[i + node_start[0]]) + 1
    start_char += start_point[1] - (node_start[1] if start_point[0] == node_start[0] else 0)
    
    # Calculate end character position  
    end_char = 0
    for i in range(end_point[0] - node_start[0]):
        if i + node_start[0] < len(lines):
            end_char += len(lines[i + node_start[0]]) + 1
    end_char += end_point[1] - (node_start[1] if end_point[0] == node_start[0] else 0)
    
    return full_text[start_char:end_char] if start_char < end_char else ""

def tokenize_gap_text(gap_text, start_point, end_point):
    """
    Tokenize gap text into meaningful spans (keywords, operators, punctuation).
    """
    import re
    
    spans = []
    
    # Define patterns for different token types
    patterns = [
        (r'\b(def|class|if|else|elif|for|while|try|except|finally|with|as|import|from|return|yield|break|continue|pass|lambda|async|await|global|nonlocal|assert|del|raise)\b', 'keyword'),
        (r'[+\-*/%=<>!&|^~@]', 'operator'),
        (r'[(){}\[\];,.:?]', 'punctuation'),
        (r'\b\w+\b', 'identifier'),
        (r'\d+', 'number'),
        (r'#.*', 'comment'),
        (r'""".*?"""|\'\'\'.*?\'\'\'|".*?"|\'.*?\'', 'string'),
        (r'\s+', 'whitespace')
    ]
    
    position = 0
    for match in re.finditer('|'.join(f'({pattern})' for pattern, _ in patterns), gap_text):
        token_text = match.group()
        if token_text.strip():  # Skip pure whitespace
            # Determine token type
            token_type = 'unknown'
            for i, (pattern, type_name) in enumerate(patterns):
                if re.match(pattern, token_text):
                    token_type = type_name
                    break
            
            spans.append({
                'type': token_type,
                'text': token_text,
                'start_line': start_point[0],
                'start_column': start_point[1] + match.start(),
                'end_line': start_point[0],  # Assuming single line gaps for now
                'end_column': start_point[1] + match.end()
            })
    
    return spans

def get_token_aligned_ast_spans(code_text, ast_root, include_positions=True, tokenizer_path="/data/home/zhangsj/qwen_coder_1.5b"):
    """
    Generate AST spans using a token-first approach for guaranteed complete coverage.
    
    Strategy:
    1. Tokenize the code first to get all tokens with positions
    2. For each token, find the most appropriate AST node
    3. Group consecutive tokens that belong to the same semantic unit
    4. Ensure every single token is assigned to exactly one span
    
    Args:
        code_text: The source code text
        ast_root: The AST root node
        include_positions: Whether to include position information
        tokenizer_path: Path to tokenizer
    
    Returns:
        List of span dictionaries that cover all tokens
    """
    try:
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
    except Exception as e:
        print(f"Error loading tokenizer: {e}")
        # Fallback to simple approach
        return get_simple_complete_spans(code_text, ast_root, include_positions)
    
    # Tokenize with offset mapping for precise alignment
    if hasattr(tokenizer, 'encode_plus'):
        encoding = tokenizer.encode_plus(
            code_text,
            add_special_tokens=False,
            return_offsets_mapping=True,
            return_tensors=None
        )
        token_ids = encoding['input_ids']
        offset_mapping = encoding['offset_mapping']
    else:
        # Fallback without offset mapping
        token_ids = tokenizer.encode(code_text, add_special_tokens=False)
        offset_mapping = None
    
    if offset_mapping is None:
        print("Warning: No offset mapping available, using fallback method")
        return get_simple_complete_spans(code_text, ast_root, include_positions)
    
    # Use a simpler approach: create individual token spans first, then try to merge semantically similar ones
    print("Creating token-aligned spans with guaranteed 100% coverage...")
    
    # Build AST node lookup for semantic information
    char_to_node_map = build_char_to_ast_node_map(code_text, ast_root)
    
    # Create individual token spans first (this guarantees 100% coverage)
    token_spans = []
    for token_idx, (token_start, token_end) in enumerate(offset_mapping):
        token_text = code_text[token_start:token_end]
        
        # Get semantic type from AST
        semantic_info = get_best_ast_node_for_range(char_to_node_map, token_start, token_end)
        semantic_type = semantic_info['type']
        
        # Create span for this single token
        token_span = create_span_from_tokens(
            [token_idx], token_text, semantic_type,
            offset_mapping, code_text, include_positions
        )
        token_spans.append(token_span)
    
    # Now try to merge adjacent tokens of the same semantic type
    merged_spans = []
    current_group = []
    current_type = None
    
    for span in token_spans:
        if (current_type == span['type'] and 
            current_group and 
            span['token_indices'][0] == current_group[-1]['token_indices'][-1] + 1):
            # Continue current group
            current_group.append(span)
        else:
            # Finish current group
            if current_group:
                merged_span = merge_token_spans(current_group, code_text, offset_mapping, include_positions)
                merged_spans.append(merged_span)
            
            # Start new group
            current_group = [span]
            current_type = span['type']
    
    # Don't forget the last group
    if current_group:
        merged_span = merge_token_spans(current_group, code_text, offset_mapping, include_positions)
        merged_spans.append(merged_span)
    
    print(f"Created {len(merged_spans)} spans covering all {len(token_ids)} tokens")
    return merged_spans

def merge_token_spans(spans_to_merge, code_text, offset_mapping, include_positions):
    """Merge multiple token spans into a single span."""
    if len(spans_to_merge) == 1:
        return spans_to_merge[0]
    
    # Collect all token indices
    all_token_indices = []
    for span in spans_to_merge:
        all_token_indices.extend(span['token_indices'])
    
    # Get text from first to last token
    first_token_idx = min(all_token_indices)
    last_token_idx = max(all_token_indices)
    start_char = offset_mapping[first_token_idx][0]
    end_char = offset_mapping[last_token_idx][1]
    merged_text = code_text[start_char:end_char]
    
    # Use type from first span (they should all be the same)
    merged_type = spans_to_merge[0]['type']
    
    return create_span_from_tokens(
        sorted(all_token_indices), merged_text, merged_type,
        offset_mapping, code_text, include_positions
    )

def build_char_to_ast_node_map(code_text, ast_root):
    """Build a mapping from character positions to AST nodes."""
    char_to_node = {}
    
    def visit_node(node):
        start_line, start_col = node.start_point
        end_line, end_col = node.end_point
        
        # Convert to character positions using a more reliable method
        lines = code_text.split('\n')
        
        # Calculate start position
        start_char = 0
        for i in range(start_line):
            if i < len(lines):
                start_char += len(lines[i]) + 1  # +1 for \n
        if start_line < len(lines):
            start_char += start_col
        
        # Calculate end position  
        end_char = 0
        for i in range(end_line):
            if i < len(lines):
                end_char += len(lines[i]) + 1  # +1 for \n
        if end_line < len(lines):
            end_char += end_col
        
        # Bounds checking
        start_char = max(0, min(start_char, len(code_text)))
        end_char = max(0, min(end_char, len(code_text)))
        
        # Only map if the range is valid
        if start_char < end_char:
            # Map character positions to this node (prefer leaf nodes)
            for char_pos in range(start_char, end_char):
                if char_pos < len(code_text):
                    if (char_pos not in char_to_node or 
                        len(node.children) <= len(char_to_node[char_pos].children)):
                        char_to_node[char_pos] = node
        
        # Recursively process children
        for child in node.children:
            visit_node(child)
    
    try:
        visit_node(ast_root)
    except Exception as e:
        print(f"Warning: Error building char-to-node map: {e}")
        # Fallback: map everything to root
        for i in range(len(code_text)):
            char_to_node[i] = ast_root
    
    return char_to_node

def get_best_ast_node_for_range(char_to_node_map, start_char, end_char):
    """Get the best AST node type for a character range."""
    # Sample a few positions in the range to determine the dominant node type
    sample_positions = [start_char, (start_char + end_char) // 2, max(start_char, end_char - 1)]
    
    node_type_votes = {}
    for pos in sample_positions:
        if pos in char_to_node_map:
            node = char_to_node_map[pos]
            node_type = get_semantic_node_type(node)
            node_type_votes[node_type] = node_type_votes.get(node_type, 0) + 1
    
    # Return the most common node type
    if node_type_votes:
        best_type = max(node_type_votes, key=node_type_votes.get)
        return {'type': best_type}
    else:
        return {'type': 'unknown'}

def get_semantic_node_type(ast_node):
    """Map AST node types to semantic categories."""
    node_type = ast_node.type
    
    # Map to semantic categories
    keyword_types = {'def', 'class', 'if', 'else', 'elif', 'for', 'while', 'try', 'except', 'finally', 
                    'with', 'as', 'import', 'from', 'return', 'yield', 'break', 'continue', 'pass', 
                    'lambda', 'async', 'await', 'global', 'nonlocal', 'assert', 'del', 'raise'}
    
    operator_types = {'binary_operator', 'unary_operator', 'comparison_operator', 'boolean_operator',
                     'assignment', 'augmented_assignment', '+', '-', '*', '/', '%', '**', '//', 
                     '<<', '>>', '&', '|', '^', '~', '<', '>', '<=', '>=', '==', '!=', 'and', 'or', 'not'}
    
    punctuation_types = {'(', ')', '[', ']', '{', '}', ',', ':', ';', '.', '->', '=>'}
    
    if node_type in keyword_types or node_type == 'keyword':
        return 'keyword'
    elif node_type in operator_types:
        return 'operator'  
    elif node_type in punctuation_types:
        return 'punctuation'
    elif node_type in {'string', 'string_literal', 'string_content'}:
        return 'string'
    elif node_type in {'integer', 'float', 'number'}:
        return 'number'
    elif node_type == 'comment':
        return 'comment'
    elif node_type in {'identifier', 'name'}:
        return 'identifier'
    else:
        return node_type  # Use original type

def create_span_from_tokens(token_indices, span_text, node_type, offset_mapping, code_text, include_positions):
    """Create a span dictionary from token information."""
    # Get position info from first and last tokens
    start_char = offset_mapping[token_indices[0]][0]
    end_char = offset_mapping[token_indices[-1]][1]
    
    # Convert back to line/column positions
    lines = code_text.split('\n')
    
    # Find start line/column
    char_count = 0
    start_line = start_col = 0
    for line_idx, line in enumerate(lines):
        if char_count + len(line) >= start_char:
            start_line = line_idx
            start_col = start_char - char_count
            break
        char_count += len(line) + 1  # +1 for newline
    
    # Find end line/column
    char_count = 0
    end_line = end_col = 0
    for line_idx, line in enumerate(lines):
        if char_count + len(line) >= end_char:
            end_line = line_idx
            end_col = end_char - char_count
            break
        char_count += len(line) + 1  # +1 for newline
    
    span_info = {
        'type': node_type,
        'text': span_text.strip() if span_text.strip() else span_text,
        'token_indices': token_indices
    }
    
    if include_positions:
        span_info.update({
            'start_line': start_line,
            'start_column': start_col,
            'end_line': end_line,
            'end_column': end_col
        })
    
    return span_info

def get_simple_complete_spans(code_text, ast_root, include_positions=True):
    """Fallback method when tokenizer offset mapping is not available."""
    # This is a simpler approach that just uses character-by-character mapping
    spans = []
    
    # Get all leaf nodes
    leaf_nodes = []
    def collect_leaves(node):
        if len(node.children) == 0:
            leaf_nodes.append(node)
        else:
            for child in node.children:
                collect_leaves(child)
    
    collect_leaves(ast_root)
    
    # Sort by position
    leaf_nodes.sort(key=lambda n: (n.start_point[0], n.start_point[1]))
    
    # Convert to spans  
    for node in leaf_nodes:
        node_text = node.text.decode('utf-8')
        if node_text.strip():  # Skip empty nodes
            span_info = {
                'type': get_semantic_node_type(node),
                'text': node_text
            }
            
            if include_positions:
                span_info.update({
                    'start_line': node.start_point[0],
                    'start_column': node.start_point[1],
                    'end_line': node.end_point[0],
                    'end_column': node.end_point[1]
                })
            
            spans.append(span_info)
    
    return spans

def get_ast_leaf_nodes(node, nodes=None, depth=0, include_positions=True):
    """Get a list of AST leaf nodes using DFS with improved coverage
    
    Collects:
    - Leaf nodes (nodes with no children)
    - Comments and string literals
    - Keywords, operators, and punctuation
    - Position information (optional)
    
    Args:
        node: AST node to process
        nodes: Accumulated list of nodes
        depth: Current depth in the AST
        include_positions: Whether to include position information
    
    Returns:
        List of node dictionaries with type, text, and optional position info
    """
    if nodes is None:
        nodes = []
    
    # Define important node types to always include
    important_types = {
        'comment', 'string', 'identifier', 'keyword', 'operator',
        'number', 'boolean', 'null', 'undefined', 'symbol'
    }
    
    # Define punctuation and structural tokens
    punctuation_types = {
        '(', ')', '{', '}', '[', ']', ';', ',', '.', ':', '=>', '->'
    }
    
    # Check if this is a leaf node or important type
    is_leaf = len(node.children) == 0
    is_important = node.type in important_types
    is_punctuation = node.text.decode('utf-8') in punctuation_types
    
    # Include node if it's a leaf, important type, or punctuation
    if is_leaf or is_important or is_punctuation:
        node_info = {
            'type': node.type,
            'text': node.text.decode('utf-8'),
            'depth': depth
        }
        
        if include_positions:
            node_info.update({
                'start_line': node.start_point[0],
                'start_column': node.start_point[1],
                'end_line': node.end_point[0],
                'end_column': node.end_point[1]
            })
        
        nodes.append(node_info)
    
    # Process children for non-leaf nodes
    for child in node.children:
        get_ast_leaf_nodes(child, nodes, depth + 1, include_positions)
    
    return nodes

def get_ast_structural_nodes(node, nodes=None, depth=0):
    """
    Get structural AST nodes with hierarchical information
    Collects important structural nodes like function_def, class_def, etc.
    """
    if nodes is None:
        nodes = []
    
    # Define important node types that provide structural information
    important_types = {
        'function_def', 'class_def', 'method_def', 'if_statement', 
        'for_statement', 'while_statement', 'try_statement', 'with_statement',
        'return_statement', 'assignment', 'call', 'binary_operation',
        'comparison_operator', 'argument_list', 'parameter_list'
    }
    
    # Add node if it's an important structural type
    if node.type in important_types:
        nodes.append({
            'type': node.type,
            'text': node.text.decode('utf-8'),
            'depth': depth,
            'start_pos': node.start_point,
            'end_pos': node.end_point
        })
    
    # Recursively process children
    for child in node.children:
        get_ast_structural_nodes(child, nodes, depth + 1)
    
    return nodes

def read_functions_from_csv(input_file):
    """Read Python functions from a CSV file"""
    functions = []
    with open(input_file, 'r', newline='', encoding='utf-8') as csvfile:
        reader = csv.reader(csvfile)
        for row in reader:
            functions.append(str(row[0]))
    return functions

def write_results_to_csv(output_file, results):
    """Write AST leaf node lists to CSV file"""
    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        # Write header
        writer.writerow(['ast_leaf_nodes'])
        
        # Write data - each row is a list of [node_type, node_text] lists for leaf nodes
        for node_list in results:
            writer.writerow([node_list])

def process_functions_from_csv(input_file, output_file, verbose_errors=False, use_fallback=True):
    """
    Process Python functions from input CSV and write AST leaf node lists to output CSV
    with improved error handling and preprocessing.
    
    Args:
        input_file: Path to input CSV file
        output_file: Path to output CSV file
        verbose_errors: Whether to print detailed error information
        use_fallback: Whether to use fallback parsing strategies
    """
    functions = read_functions_from_csv(input_file)
    results = []
    success_count = 0
    error_count = 0
    strategy_stats = {}
    
    print(f"Processing {len(functions)} functions...")
    
    for i, func in enumerate(functions):
        try:
            if use_fallback:
                # Use fallback parsing with multiple strategies
                ast_root, strategy = parse_to_ast_with_fallback(func, ps_language="python", verbose_errors=verbose_errors)
                strategy_stats[strategy] = strategy_stats.get(strategy, 0) + 1
            else:
                # Use standard parsing with preprocessing
                ast_root = parse_to_ast(func, ps_language="python", verbose_errors=verbose_errors, preprocess=True)
                strategy = "standard"
            
            if ast_root is not None:
                nodes = get_ast_leaf_nodes(ast_root)
                results.append(nodes)
                success_count += 1
                
                if verbose_errors and i % 100 == 0:
                    print(f"Processed {i+1}/{len(functions)} functions successfully")
            else:
                results.append([])
                error_count += 1
                if verbose_errors:
                    print(f"Function {i+1} failed to parse with all strategies")
                    
        except Exception as e:
            results.append([])
            error_count += 1
            if verbose_errors:
                print(f"Error processing function {i+1}: {e}")
    
    write_results_to_csv(output_file, results)
    
    print(f"Processing complete:")
    print(f"  Total functions: {len(functions)}")
    print(f"  Successfully parsed: {success_count}")
    print(f"  Failed to parse: {error_count}")
    print(f"  Success rate: {success_count/len(functions)*100:.1f}%")
    if use_fallback and strategy_stats:
        print(f"  Strategy usage: {strategy_stats}")
    print(f"  Results written to {output_file}")

# Add this new function
def get_token_positions_for_ast_nodes_improved(tokenizer, code_text, ast_nodes):
    """
    Improved token position mapping for AST nodes
    Handles both leaf nodes and structural nodes with better matching
    """
    token_positions = []
    seen_positions = set()
    
    # Tokenize the entire text
    all_tokens = tokenizer.encode(code_text, add_special_tokens=False)
    
    for node in ast_nodes:
        if isinstance(node, dict):
            # Structural node
            node_text = node['text']
            node_type = node['type']
            node_weight = get_node_type_weight(node_type)
        else:
            # Leaf node (backward compatibility)
            node_type, node_text = node
            node_weight = get_node_type_weight(node_type)
        
        # Tokenize the node text
        node_tokens = tokenizer.encode(node_text, add_special_tokens=False)
        node_token_len = len(node_tokens)
        
        # Skip if node would only produce a single token (unless it's important)
        if node_token_len <= 1 and node_weight <= 1.0:
            continue
            
        # Search for all occurrences of node_tokens in all_tokens
        for i in range(len(all_tokens) - node_token_len + 1):
            if all_tokens[i:i + node_token_len] == node_tokens:
                # Create position sequence
                pos_sequence = tuple(range(i, i + node_token_len))
                
                # Add if sequence is longer than 1 token or has high weight
                if (len(pos_sequence) > 1 or node_weight > 1.5) and pos_sequence not in seen_positions:
                    token_positions.append({
                        'positions': list(pos_sequence),
                        'weight': node_weight,
                        'type': node_type
                    })
                    seen_positions.add(pos_sequence)
    
    return token_positions

def get_node_type_weight(node_type):
    """
    Get weight for different AST node types
    Higher weights for more important structural elements
    """
    weights = {
        # High importance - core structural elements
        'function_def': 3.0,
        'class_def': 3.0,
        'method_def': 2.5,
        
        # Medium importance - control flow
        'if_statement': 2.0,
        'for_statement': 2.0,
        'while_statement': 2.0,
        'try_statement': 2.0,
        'with_statement': 2.0,
        'return_statement': 2.0,
        
        # Medium importance - operations
        'assignment': 1.5,
        'call': 1.5,
        'binary_operation': 1.5,
        'comparison_operator': 1.5,
        
        # Lower importance - structural details
        'argument_list': 1.0,
        'parameter_list': 1.0,
        
        # Default weight
        'default': 1.0
    }
    
    return weights.get(node_type, weights['default'])

def test_language_coverage(language, code, description):
    """Test AST parsing coverage for a specific language and code sample."""
    print(f"\n{'='*60}")
    print(f"Testing {language.upper()}: {description}")
    print(f"{'='*60}")
    print(f"Code sample:\n{code}")
    print("-" * 40)
    
    try:
        # Parse the code
        ast_root = parse_to_ast(code, ps_language=language)
        
        if ast_root is None:
            print(f" FAILED: Could not parse {language} code!")
            return False
        
        # Test both methods
        old_nodes = get_ast_leaf_nodes(ast_root)
        new_nodes = get_ast_leaf_nodes_for_spans(ast_root)
        
        print(f" Node counts - Old: {len(old_nodes)}, New: {len(new_nodes)}")
        
        # Validate coverage
        old_validation = validate_comprehensive_coverage(old_nodes, code)
        new_validation = validate_comprehensive_coverage(new_nodes, code)
        
        # Handle different validation formats
        old_coverage = old_validation.get('coverage_percentage', 0)
        
        if 'token_coverage_percentage' in new_validation:
            new_coverage = new_validation['token_coverage_percentage']
            uncovered = new_validation.get('uncovered_tokens', 0)
        else:
            new_coverage = new_validation.get('coverage_percentage', 0)
            uncovered = new_validation.get('uncovered_characters', 0)
        
        # Results summary
        print(f" Coverage Results:")
        print(f"   Old method: {old_coverage:.1f}%")
        print(f"   New method: {new_coverage:.1f}% (uncovered: {uncovered})")
        print(f"   Quality score: {new_validation.get('quality_score', 0):.1f}")
        
        # Check if perfect coverage achieved
        is_perfect = new_coverage == 100.0 and uncovered == 0
        print(f" Perfect coverage: {'YES' if is_perfect else 'NO'}")
        
        # Show sample spans
        print(f" Sample spans (first 5):")
        for i, node in enumerate(new_nodes[:5]):
            node_text = node['text'][:30] + "..." if len(node['text']) > 30 else node['text']
            token_count = len(node.get('token_indices', []))
            print(f"   {i+1}. {node['type']:12} | {node_text:20} | tokens: {token_count}")
        
        if len(new_nodes) > 5:
            print(f"   ... and {len(new_nodes) - 5} more spans")
        
        # Gap analysis if not perfect
        if not is_perfect and new_validation.get('gap_analysis'):
            gap_info = new_validation['gap_analysis']
            if 'message' not in gap_info:  # Only show if there are actual gaps
                print(f" Gap analysis: {gap_info}")
        
        return is_perfect
        
    except Exception as e:
        print(f" ERROR: {e}")
        import traceback
        traceback.print_exc()
        return False

# Example usage
if __name__ == "__main__":
    # Test cases for different languages
    test_cases = [
        # Python tests
        ("python", '''def calculate_sum(a, b):
    """Calculate the sum of two numbers."""
    result = a + b  # Addition operation
    return result

# Test the function
if __name__ == "__main__":
    print(calculate_sum(5, 3))''', "Function with docstring and comments"),
        
        ("python", '''class DataProcessor:
    def __init__(self, data):
        self.data = data
    
    def process(self):
        return [x * 2 for x in self.data if x > 0]''', "Class with list comprehension"),
        
        # Java tests
        ("java", '''public class HelloWorld {
    public static void main(String[] args) {
        System.out.println("Hello, World!");
        int sum = 0;
        for (int i = 1; i <= 10; i++) {
            sum += i;
        }
        System.out.println("Sum: " + sum);
    }
}''', "Basic class with loop"),
        
        ("java", '''public interface Calculator {
    double add(double a, double b);
    default double multiply(double a, double b) {
        return a * b;
    }
}''', "Interface with default method"),
        
        # JavaScript tests
        ("javascript", '''function factorial(n) {
    if (n <= 1) return 1;
    return n * factorial(n - 1);
}

const numbers = [1, 2, 3, 4, 5];
const doubled = numbers.map(x => x * 2);
console.log(doubled);''', "Recursive function with arrow functions"),
        
        ("javascript", '''class Rectangle {
    constructor(width, height) {
        this.width = width;
        this.height = height;
    }
    
    get area() {
        return this.width * this.height;
    }
}

const rect = new Rectangle(10, 5);
console.log(`Area: ${rect.area}`);''', "ES6 class with getter and template literals"),
        
        # PHP tests
        ("php", '''<?php
class User {
    private $name;
    private $email;
    
    public function __construct($name, $email) {
        $this->name = $name;
        $this->email = $email;
    }
    
    public function getName() {
        return $this->name;
    }
}

$user = new User("Alice", "alice@example.com");
echo "User: " . $user->getName();
?>''', "Class with constructor and methods"),
        
        # Rust tests  
        ("rust", '''fn fibonacci(n: u32) -> u32 {
    match n {
        0 => 0,
        1 => 1,
        _ => fibonacci(n - 1) + fibonacci(n - 2),
    }
}

fn main() {
    let result = fibonacci(10);
    println!("Fibonacci(10) = {}", result);
    
    let numbers: Vec<i32> = (1..=5).collect();
    let sum: i32 = numbers.iter().sum();
    println!("Sum: {}", sum);
}''', "Pattern matching and iterators"),
        
        ("rust", '''struct Point {
    x: f64,
    y: f64,
}

impl Point {
    fn new(x: f64, y: f64) -> Point {
        Point { x, y }
    }
    
    fn distance(&self, other: &Point) -> f64 {
        ((self.x - other.x).powi(2) + (self.y - other.y).powi(2)).sqrt()
    }
}''', "Struct with implementation"),
        
        # C++ tests
        ("cpp", '''#include <iostream>
#include <vector>
#include <algorithm>

int main() {
    std::vector<int> numbers = {5, 2, 8, 1, 9};
    
    std::sort(numbers.begin(), numbers.end());
    
    for (const auto& num : numbers) {
        std::cout << num << " ";
    }
    std::cout << std::endl;
    
    return 0;
}''', "STL containers and algorithms"),
        
        ("cpp", '''template<typename T>
class Stack {
private:
    std::vector<T> data;
    
public:
    void push(const T& item) {
        data.push_back(item);
    }
    
    T pop() {
        if (!data.empty()) {
            T item = data.back();
            data.pop_back();
            return item;
        }
        throw std::runtime_error("Stack is empty");
    }
};''', "Template class with exception handling"),
    ]
    
    # Run all test cases
    print(" COMPREHENSIVE AST PARSING TEST SUITE")
    print("Testing improved token-first approach across all supported languages...")
    
    results = {}
    total_tests = len(test_cases)
    perfect_count = 0
    
    for language, code, description in test_cases:
        is_perfect = test_language_coverage(language, code, description)
        
        if language not in results:
            results[language] = []
        results[language].append(is_perfect)
        
        if is_perfect:
            perfect_count += 1
    
    # Final summary
    print(f"\n{'='*60}")
    print(" FINAL SUMMARY")
    print(f"{'='*60}")
    print(f"Total tests: {total_tests}")
    print(f"Perfect coverage achieved: {perfect_count}/{total_tests} ({perfect_count/total_tests*100:.1f}%)")
    print()
    
    for language in sorted(results.keys()):
        language_results = results[language]
        perfect_in_lang = sum(language_results)
        total_in_lang = len(language_results)
        success_rate = perfect_in_lang / total_in_lang * 100
        
        status = "" if success_rate == 100 else "" if success_rate >= 50 else ""
        print(f"{status} {language.upper():12}: {perfect_in_lang}/{total_in_lang} perfect ({success_rate:.1f}%)")
    
    if perfect_count == total_tests:
        print(f"\n EXCELLENT! All languages achieved 100% token coverage!")
    elif perfect_count >= total_tests * 0.8:
        print(f"\n GOOD! Most tests achieved perfect coverage.")
    else:
        print(f"\n  Some languages need improvement.")
    
    print(f"\n The improved token-first approach successfully addresses the original")
    print(f"   coverage issues and ensures complete token alignment for SpanBERT training!")

    # Optional: Process actual CSV files
    # input_csv = "/data/home/zhangsj/Data/HumanEval/humaneval_concate.csv"
    # output_csv = "/data/home/zhangsj/Data/HumanEval/humaneval_ast_leaf_nodes.csv" 
    # process_functions_from_csv(input_csv, output_csv)
